{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/hassanadnan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/hassanadnan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Download the dataset and tagset from below:\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_HMM:\n",
    "    def __init__(self, corpus): #DO NOT MODIFY THIS FUNCTION\n",
    "\n",
    "        #-----------------DO NOT MODIFY ANYTHING BELOW THIS LINE-----------------\n",
    "        self.corpus = corpus\n",
    "        self.train_set, self.test_set = train_test_split( self.corpus, train_size=0.85,random_state = 777)        \n",
    "        \n",
    "        # Extracting Vocabulary and Tags from our training set\n",
    "        self.all_pairs = [(word, tag) for sentence in self.train_set for word, tag in sentence] #List of tuples (word, POS tag) or a flattened list of tuples by concatenating all sentences\n",
    "        self.vocab = tuple(set(word for (word, _) in self.all_pairs))\n",
    "\n",
    "        self.all_tags = [tag for (_, tag) in self.all_pairs] #List of all POS tags in the trainset\n",
    "        self.tags = tuple(set(self.all_tags)) #List of unique POS tags\n",
    "        self.word_tag_counts = Counter(self.all_pairs)\n",
    "\n",
    "        # Mapping vocab and tags to integers for indexing\n",
    "        self.vocab2index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.tag2index = {tag: i for i, tag in enumerate(self.tags)}\n",
    "\n",
    "        self.vocab_len = len(self.vocab) #Total Number of Vocab (Unique Words)\n",
    "        self.all_tag_lengths = len(self.all_tags) #Number of tags in the trainset\n",
    "        self.tag_len = len(self.tags) #Number of unique tags\n",
    "\n",
    "        # Initialize transition and emission matrices (Default: Zeros)\n",
    "        self.transition_mat = np.zeros((self.tag_len, self.tag_len))\n",
    "        self.emission_mat = np.zeros((self.tag_len, self.vocab_len))\n",
    "        self.initial_state_prob = np.zeros(self.tag_len)\n",
    "\n",
    "        # Initialize POS Tag occurance probabilities for getting most likely POS Tags for unknown words\n",
    "        self.tag_occur_prob= {} #Dictionary of POS Tag occurance probabilities\n",
    "        all_tag_counts = Counter(self.all_tags)\n",
    "        for tag in self.tags:\n",
    "            self.tag_occur_prob[tag] = all_tag_counts[tag]/self.all_tag_lengths\n",
    "        self.tag_counts = Counter(tag for _, tag in self.all_pairs)\n",
    "        #-----------------Add additional variables here-----------------\n",
    "        def word_given_tag(word, tag):\n",
    "            count_tag = self.all_tags.count(tag)\n",
    "            count_w_given_tag = self.all_pairs.count((word, tag))\n",
    "            return count_w_given_tag / count_tag if count_tag != 0 else 0\n",
    "\n",
    "        def next_tag_given_prev_tag(tag2, tag1):\n",
    "            count_t1 = self.all_tags.count(tag1)\n",
    "            count_t2_t1 = sum(1 for i in range(len(self.all_tags) - 1) \n",
    "                              if self.all_tags[i] == tag1 and self.all_tags[i + 1] == tag2)\n",
    "            return count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
    "\n",
    "\n",
    "    def init_params(self): #Initialize transition and emission matrices via Supervised Learning (Counting Occurences of emissions and transitions observed in the data).\n",
    "        all_pairs_array = np.array(self.all_pairs)\n",
    "        tags_array = np.array(self.all_tags)\n",
    "\n",
    "        #------------------- Space provided for any additional data structures that you may need or any process that you may need to perform-------------------\n",
    "        for tag in self.tags:\n",
    "            tag_index = self.tag2index[tag]\n",
    "            self.initial_state_prob[tag_index] = self.tag_occur_prob[tag]\n",
    "\n",
    "            for next_tag in self.tags:\n",
    "                next_tag_index = self.tag2index[next_tag]\n",
    "                count_t1_t2 = sum(1 for i in range(len(self.all_pairs) - 1) \n",
    "                                  if self.all_pairs[i][1] == tag and self.all_pairs[i + 1][1] == next_tag)\n",
    "                self.transition_mat[tag_index, next_tag_index] = count_t1_t2 / self.tag_counts[tag]\n",
    "\n",
    "            for word in self.vocab:\n",
    "                word_index = self.vocab2index[word]\n",
    "                self.emission_mat[tag_index, word_index] = self.word_tag_counts[(word, tag)] / self.tag_counts[tag]\n",
    "\n",
    "        # Normalize matrices\n",
    "        self.normalize_matrices()\n",
    "\n",
    "    def normalize_matrices(self):\n",
    "        self.transition_mat /= self.transition_mat.sum(axis=1, keepdims=True)\n",
    "        self.emission_mat /= self.emission_mat.sum(axis=1, keepdims=True)\n",
    "        self.initial_state_prob /= self.initial_state_prob.sum()\n",
    "\n",
    "\n",
    "\n",
    "        def word_given_tag(word, tag):\n",
    "            count_tag = self.all_tags.count(tag)\n",
    "            count_w_given_tag = self.all_pairs.count((word, tag))\n",
    "            return count_w_given_tag / count_tag if count_tag != 0 else 0\n",
    "    \n",
    "        def next_tag_given_prev_tag(tag2, tag1):\n",
    "            count_t1 = self.all_tags.count(tag1)\n",
    "            count_t2_t1 = sum(1 for i in range(len(self.all_tags) - 1) \n",
    "                            if self.all_tags[i] == tag1 and self.all_tags[i + 1] == tag2)\n",
    "            return count_t2_t1 / count_t1 if count_t1 != 0 else 0\n",
    "        \n",
    "        \n",
    "        # Compute Transition Matrix\n",
    "        for i, t1 in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Transition Matrix\", mininterval=10)):\n",
    "            for j, t2 in enumerate(list(self.tag2index.keys())):\n",
    "                self.transition_mat[i, j] = next_tag_given_prev_tag(t2, t1)\n",
    "\n",
    "    # Compute Emission Matrix\n",
    "        for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Emission Matrix\", mininterval=10)):\n",
    "            for j, word in enumerate(list(self.vocab2index.keys())):\n",
    "                self.emission_mat[i, j] = word_given_tag(word, tag)\n",
    "        \n",
    "        for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Initial Probability Matrix\", mininterval=10)):\n",
    "            self.initial_state_prob[i] = self.all_tags.count(tag) / len(self.all_tags) if len(self.all_tags) != 0 else 0\n",
    "\n",
    "        # Normalize matrices\n",
    "        self.transition_mat = self.transition_mat / self.transition_mat.sum(axis=1, keepdims=True)\n",
    "        self.emission_mat = self.emission_mat / self.emission_mat.sum(axis=1, keepdims=True)\n",
    "        self.initial_state_prob = self.initial_state_prob / self.initial_state_prob.sum() if self.initial_state_prob.sum() != 0 else self.initial_state_prob\n",
    "        # Compute Initial State Probability\n",
    "                \n",
    "        #The below code may help. You can modify it as per your requirement.\n",
    "        #for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Initial Probability Matrix\", mininterval = 10)):\n",
    "        #    self.initial_state_prob[i] = None\n",
    "\n",
    "        \n",
    "        # Normalize matrices i.e. each row sums to 1\n",
    "                \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def viterbi_decoding(self, sentence): #Sentence is a list words i.e. [\"Moon\", \"Landing\", \"was\", \"Faked\"]\n",
    "        \n",
    "        pred_pos_sequence = []  # Implement the Viterbi Algorithm to predict the POS tags of the given sentence\n",
    "\n",
    "        # Number of words in the sentence and number of tags\n",
    "        n_words = len(sentence)\n",
    "        n_tags = len(self.tags)\n",
    "\n",
    "        # Probability matrix, initialized with zeros\n",
    "        # Each cell prob_matrix[t][i] holds the probability of the most probable tag sequence ending in tag t for the first i words in the sentence.\n",
    "        prob_matrix = np.zeros((n_tags, n_words))\n",
    "\n",
    "        # Back pointers matrix to reconstruct the path of tags\n",
    "        back_pointers = np.zeros((n_tags, n_words), dtype=int)\n",
    "\n",
    "        # Initialization step (i=0 for the first word in the sentence)\n",
    "        for t in range(n_tags):\n",
    "            # Probability of tag 't' being the first tag and the first word being emitted by tag 't'\n",
    "            prob_matrix[t][0] = self.initial_state_prob[t] * self.emission_mat[t][self.vocab2index.get(sentence[0], -1)]\n",
    "            back_pointers[t][0] = 0\n",
    "\n",
    "        # Dynamic programming forward pass\n",
    "        for i in range(1, n_words):\n",
    "            for t in range(n_tags):\n",
    "                # Calculate the maximum probability for each tag 't' at position 'i' in the sentence\n",
    "                # And also find the tag from the previous position that contributes to this maximum probability\n",
    "                max_prob, max_state = max((prob_matrix[t_prev][i - 1] * self.transition_mat[t_prev][t] * self.emission_mat[t][self.vocab2index.get(sentence[i], -1)], t_prev) for t_prev in range(n_tags))\n",
    "                prob_matrix[t][i] = max_prob\n",
    "                back_pointers[t][i] = max_state\n",
    "\n",
    "        # Find the final best path through backtracking\n",
    "        best_path = []\n",
    "        # Start with the most probable last tag\n",
    "        max_prob_last_tag = np.argmax(prob_matrix[:, n_words - 1])\n",
    "        best_path.append(max_prob_last_tag)\n",
    "\n",
    "        # Follow the back pointers to retrieve the best path\n",
    "        for i in range(n_words - 1, 0, -1):\n",
    "            best_tag_prev = back_pointers[best_path[-1]][i]\n",
    "            best_path.append(best_tag_prev)\n",
    "\n",
    "        # Reverse the path to get it in the correct order\n",
    "        best_path.reverse()\n",
    "\n",
    "        # Convert tag indices back to tag names\n",
    "        pred_pos_sequence = [list(self.tag2index.keys())[tag_index] for tag_index in best_path]\n",
    "\n",
    "        return pred_pos_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return pred_pos_sequence\n",
    "    \n",
    "   \n",
    "    def evaluation(self, debug=False): #DO NOT MODIFY THIS FUNCTION\n",
    "        # Evaluate the model on the test set\n",
    "        correct, total = 0, 0\n",
    "        pred_pos_sequences = []\n",
    "\n",
    "        for test_sentence in self.test_set:\n",
    "            test_sentence_words, test_sentence_tags = zip(*test_sentence)\n",
    "            pred_pos_tags = self.viterbi_decoding(test_sentence_words)\n",
    "            pred_pos_sequences.extend(pred_pos_tags)\n",
    "\n",
    "            correct += sum(1 for true_tag, pred_tag in zip(test_sentence_tags, pred_pos_tags) if true_tag == pred_tag)\n",
    "            total += len(test_sentence_words)\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 1\n",
    "\n",
    "        if debug:\n",
    "            test_words, test_tags = zip(*[(word, tag) for test_sentence in self.test_set for word, tag in test_sentence])\n",
    "            print(f\"Sentence (first 20 words): {test_words[:20]}\")\n",
    "            print(f\"True POS Tags (first 20 words): {test_tags[:20]}\")\n",
    "            print(f\"Predicted POS Tags (first 20 words): {pred_pos_sequences[:20]}\")\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Populating Transition Matrix: 100%|██████████| 12/12 [00:01<00:00, 10.32it/s]\n",
      "Populating Emission Matrix: 100%|██████████| 12/12 [18:28<00:00, 92.39s/it]\n",
      "Populating Initial Probability Matrix: 100%|██████████| 12/12 [00:00<00:00, 493.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.1660%\n"
     ]
    }
   ],
   "source": [
    "pos_hmm = POS_HMM(corpus = conll2000.tagged_sents(tagset='universal'))\n",
    "pos_hmm.init_params()\n",
    "pos_hmm.evaluation(debug = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
